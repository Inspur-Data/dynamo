ARG BASE_IMAGE="nvcr.io/nvidia/cuda-dl-base"
ARG BASE_IMAGE_TAG="25.01-cuda12.8-devel-ubuntu24.04"

FROM ${BASE_IMAGE}:${BASE_IMAGE_TAG} AS base

RUN apt-get update -y && \
    apt-get install -y \
    # NIXL build dependencies
    cmake \
    meson \
    ninja-build \
    pybind11-dev \
    # Rust build dependencies
	clang \
    libclang-dev \
	git \
    # Install utilities
    nvtop \
    tmux \
    vim \
    autoconf \
    libtool

# Install uv and create virtualenv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
RUN mkdir /opt/dynamo && \
    uv venv /opt/dynamo/venv --python 3.12

# Activate virtual environment
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"

WORKDIR /workspace

# Install patched vllm - keep this early in Dockerfile to avoid
# rebuilds from unrelated source code changes
ARG VLLM_REF="0.8.4"
ARG VLLM_PATCH="vllm_v${VLLM_REF}-dynamo-kv-disagg-patch.patch"
ARG VLLM_PATCHED_PACKAGE_NAME="ai_dynamo_vllm"
ARG VLLM_PATCHED_PACKAGE_VERSION="0.8.4.post1"
ARG VLLM_MAX_JOBS=4
RUN --mount=type=bind,source=./container/deps/,target=/tmp/deps \
    --mount=type=cache,target=/root/.cache/uv \
    mkdir /tmp/vllm && \
    uv pip install pip wheel && \
    # NOTE: vLLM build from source on ARM can take several hours, see VLLM_MAX_JOBS details.
    # PyTorch 2.7 supports CUDA 12.8 and aarch64 installs
    # NIXL has a torch dependency, so need to force-reinstall to install the correct version
    uv pip install torch==2.7.0 torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/cu128 && \
    # Download vLLM source with version matching patch
    git clone --branch v${VLLM_REF} --depth 1 https://github.com/vllm-project/vllm.git /tmp/vllm/vllm-${VLLM_REF} && \
    cd /tmp/vllm/vllm-${VLLM_REF}/ && \
    # Patch vLLM source with dynamo additions
    patch -p1 < /tmp/deps/vllm/${VLLM_PATCH} && \
    # WAR: Set package version check to 'vllm' instead of 'ai_dynamo_vllm' to avoid
    # platform detection issues on ARM install.
    # TODO: Rename package from vllm to ai_dynamo_vllm like x86 path below to remove this WAR.
    sed -i 's/version("ai_dynamo_vllm")/version("vllm")/g' vllm/platforms/__init__.py && \
    # Remove pytorch from vllm install dependencies
    python use_existing_torch.py && \
    # Build/install vllm from source
    uv pip install -r requirements/build.txt && \
    # MAX_JOBS set to avoid running OOM on vllm-flash-attn build, this can
    # significantly impact the overall build time. Each job can take up
    # to -16GB RAM each, so tune according to available system memory.
    MAX_JOBS=${VLLM_MAX_JOBS} uv pip install -vv . --no-build-isolation ; 
