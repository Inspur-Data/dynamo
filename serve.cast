{"version": 2, "width": 0, "height": 0, "timestamp": 1747580538, "env": {"SHELL": "/bin/bash", "TERM": "xterm"}}
[0.097939, "o", "\u001b[?2004h\u001b]0;root@a4u8g-0057: /workspace\u0007root@a4u8g-0057:/workspace# "]
[13.425379, "o", "cd examples/llm\r\n\u001b[?2004l\r"]
[13.425437, "o", "\u001b[?2004h\u001b]0;root@a4u8g-0057: /workspace/examples/llm\u0007root@a4u8g-0057:/workspace/examples/llm# "]
[56.54748, "o", "dynamo serve graphs.agg:Frontend -f configs/agg_tp_2_dp_4.yaml\r\n\u001b[?2004l\r"]
[58.185951, "o", "\u001b[33mUsage: \u001b[0mdynamo serve [OPTIONS] DYNAMO_PIPELINE\r\n"]
[58.186252, "o", "\u001b[2mTry \u001b[0m\u001b[2;34m'dynamo serve \u001b[0m\u001b[1;2;34m-h\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\r\n"]
[58.186768, "o", "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\r\n\u001b[31m│\u001b[0m Invalid value for '\u001b[1;36m-\u001b[0m\u001b[1;36m-config\u001b[0m\u001b[1;36m-file\u001b[0m' / '\u001b[1;32m-f\u001b[0m': Path 'configs/agg_tp_2_dp_4.yaml'  \u001b[31m│\u001b[0m\r\n\u001b[31m│\u001b[0m does not exist.                                                              \u001b[31m│\u001b[0m\r\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\r\n"]
[58.315064, "o", "\u001b[?2004h\u001b]0;root@a4u8g-0057: /workspace/examples/llm\u0007root@a4u8g-0057:/workspace/examples/llm# "]
[69.089505, "o", "dynamo serve graphs.agg:Frontend -f configs/agg_tp_1_dp_4.yaml\r\n\u001b[?2004l\r"]
[69.911278, "o", "\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve.serve\u001b[0m\u001b[2m:\u001b[0m Running dynamo serve with service configs {'Common': {'model': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', 'block-size': 64, 'max-model-len': 16384}, 'Frontend': {'served_model_name': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', 'endpoint': 'dynamo.Processor.chat/completions', 'port': 8000}, 'Processor': {'router': 'round-robin', 'common-configs': ['model', 'block-size', 'max-model-len']}, 'VllmWorker': {'enforce-eager': True, 'max-num-batched-tokens': 16384, 'enable-prefix-caching': True, 'ServiceArgs': {'workers': 4, 'resources': {'gpu': 1}}, 'common-configs': ['model', 'block-size', 'max-model-len']}, 'Planner': {'environment': 'local', 'no-operation': True}}   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Loading service from import string: graphs.agg:Frontend   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Working directory: .   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Changing working directory to: /workspace/examples/llm   \r\n"]
[69.911471, "o", "\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Adding /workspace/examples/llm to sys.path   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Parsed import string - path: graphs.agg, attributes: Frontend   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Importing from module name: graphs.agg   \r\n\u001b[2m2025-05-18T15:03:28.301Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Attempting to import module: graphs.agg   \r\n"]
[77.59074, "o", "\u001b[2m2025-05-18T15:03:35.980Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[77.592116, "o", "\u001b[2m2025-05-18T15:03:35.982Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[77.594434, "o", "\u001b[2m2025-05-18T15:03:35.984Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[77.598079, "o", "\u001b[2m2025-05-18T15:03:35.987Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[77.599228, "o", "\u001b[2m2025-05-18T15:03:35.989Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[77.601287, "o", "\u001b[2m2025-05-18T15:03:35.991Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[77.991085, "o", "\u001b[2m2025-05-18T15:03:36.380Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[79.662032, "o", "\u001b[2m2025-05-18T15:03:38.051Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Navigating attributes: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.051Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Getting attribute: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.052Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Removing /workspace/examples/llm from sys.path   \r\n\u001b[2m2025-05-18T15:03:38.052Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Restoring working directory to: /workspace/examples/llm   \r\n"]
[79.662082, "o", "\u001b[2m2025-05-18T15:03:38.052Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve.serve\u001b[0m\u001b[2m:\u001b[0m Loaded service: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.052Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve.serve\u001b[0m\u001b[2m:\u001b[0m Dependencies: ['Processor', 'VllmWorker']   \r\n"]
[79.74675, "o", "\u001b[32m╭─\u001b[0m\u001b[32m───────────────\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mDynamo Serve\u001b[0m\u001b[32m \u001b[0m\u001b[32m───────────────\u001b[0m\u001b[32m─╮\u001b[0m\r\n\u001b[32m│\u001b[0m \u001b[1mStarting Dynamo service:\u001b[0m \u001b[36mgraphs.agg:Frontend\u001b[0m \u001b[32m│\u001b[0m\r\n\u001b[32m╰──────────────────────────────────────────────╯\u001b[0m\r\n"]
[79.747556, "o", "\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Loading service from import string: graphs.agg:Frontend   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Working directory: .   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Changing working directory to: /workspace/examples/llm   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Adding /workspace/examples/llm to sys.path   \r\n"]
[79.747689, "o", "\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Parsed import string - path: graphs.agg, attributes: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Importing from module name: graphs.agg   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Attempting to import module: graphs.agg   \r\n"]
[79.74774, "o", "\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Navigating attributes: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader._do_import\u001b[0m\u001b[2m:\u001b[0m Getting attribute: Frontend   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Removing /workspace/examples/llm from sys.path   \r\n\u001b[2m2025-05-18T15:03:38.137Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.find_and_load_service\u001b[0m\u001b[2m:\u001b[0m Restoring working directory to: /workspace/examples/llm   \r\n"]
[79.787699, "o", "\u001b[2m2025-05-18T15:03:38.177Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mresource._discover_gpus\u001b[0m\u001b[2m:\u001b[0m Discovered 4 GPUs   \r\n"]
[79.830602, "o", "\u001b[2m2025-05-18T15:03:38.220Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mresource._discover_gpus\u001b[0m\u001b[2m:\u001b[0m Discovered 4 GPUs   \r\n"]
[79.830729, "o", "\u001b[2m2025-05-18T15:03:38.220Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Getting resource envs for service Frontend   \r\n"]
[79.830815, "o", "\u001b[2m2025-05-18T15:03:38.220Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Using configured worker count: 1   \r\n\u001b[2m2025-05-18T15:03:38.220Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Final resource allocation - workers: 1, envs: []   \r\n"]
[79.831121, "o", "\u001b[2m2025-05-18T15:03:38.221Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Getting resource envs for service Processor   \r\n"]
[79.831351, "o", "\u001b[2m2025-05-18T15:03:38.221Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Using configured worker count: 1   \r\n\u001b[2m2025-05-18T15:03:38.221Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Final resource allocation - workers: 1, envs: []   \r\n"]
[79.836596, "o", "\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving.create_dynamo_watcher\u001b[0m\u001b[2m:\u001b[0m Created watcher for Processor's in the dynamo namespace   \r\n\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Getting resource envs for service VllmWorker   \r\n"]
[79.836762, "o", "\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU requirement found: 1   \r\n\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Using configured worker count: 4   \r\n\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU allocation enabled   \r\n\u001b[2m2025-05-18T15:03:38.226Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Local deployment detected. Allocating GPUs for 4 workers of 'VllmWorker'   \r\n"]
[79.841002, "o", "\u001b[2m2025-05-18T15:03:38.230Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU 0 (NVIDIA L40): Memory: 44.3GB free / 45.0GB total, Utilization: 0%, Temperature: 28°C   \r\n"]
[79.84485, "o", "\u001b[2m2025-05-18T15:03:38.234Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU 1 (NVIDIA L40): Memory: 44.3GB free / 45.0GB total, Utilization: 0%, Temperature: 27°C   \r\n"]
[79.848698, "o", "\u001b[2m2025-05-18T15:03:38.238Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU 2 (NVIDIA L40): Memory: 44.3GB free / 45.0GB total, Utilization: 0%, Temperature: 27°C   \r\n"]
[79.852524, "o", "\u001b[2m2025-05-18T15:03:38.242Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m GPU 3 (NVIDIA L40): Memory: 44.3GB free / 45.0GB total, Utilization: 0%, Temperature: 27°C   \r\n\u001b[2m2025-05-18T15:03:38.242Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mallocator.get_resource_envs\u001b[0m\u001b[2m:\u001b[0m Final resource allocation - workers: 4, envs: [{'CUDA_VISIBLE_DEVICES': '0'}, {'CUDA_VISIBLE_DEVICES': '1'}, {'CUDA_VISIBLE_DEVICES': '2'}, {'CUDA_VISIBLE_DEVICES': '3'}]   \r\n"]
[79.85274, "o", "\u001b[2m2025-05-18T15:03:38.242Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving.create_dynamo_watcher\u001b[0m\u001b[2m:\u001b[0m Created watcher for VllmWorker's in the dynamo namespace   \r\n"]
[79.853236, "o", "\u001b[2m2025-05-18T15:03:38.243Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving.serve_dynamo_graph\u001b[0m\u001b[2m:\u001b[0m Created watcher for Frontend with 1 workers in the dynamo namespace   \r\n"]
[79.85353, "o", "\u001b[2m2025-05-18T15:03:38.243Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2marbiter._ensure_ioloop\u001b[0m\u001b[2m:\u001b[0m Installing handle_callback_exception to loop   \r\n\u001b[2m2025-05-18T15:03:38.243Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2msighandler.__init__\u001b[0m\u001b[2m:\u001b[0m Registering signals...   \r\n"]
[79.854217, "o", "\u001b[2m2025-05-18T15:03:38.244Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2marbiter.start\u001b[0m\u001b[2m:\u001b[0m Starting master on pid 454   \r\n"]
[79.856427, "o", "\u001b[2m2025-05-18T15:03:38.246Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2marbiter.initialize\u001b[0m\u001b[2m:\u001b[0m sockets started   \r\n"]
[79.893725, "o", "\u001b[2m2025-05-18T15:03:38.283Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2marbiter.start\u001b[0m\u001b[2m:\u001b[0m Arbiter now waiting for commands   \r\n"]
[79.893865, "o", "\u001b[2m2025-05-18T15:03:38.283Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._start\u001b[0m\u001b[2m:\u001b[0m dynamo_Processor started   \r\n"]
[80.022701, "o", "\u001b[2m2025-05-18T15:03:38.412Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._start\u001b[0m\u001b[2m:\u001b[0m dynamo_VllmWorker started   \r\n"]
[80.053988, "o", "\u001b[2m2025-05-18T15:03:38.443Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._start\u001b[0m\u001b[2m:\u001b[0m dynamo_Frontend started   \r\n"]
[80.054128, "o", "\u001b[2m2025-05-18T15:03:38.444Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving.<lambda>\u001b[0m\u001b[2m:\u001b[0m Starting Dynamo Service Frontend (Press CTRL+C to quit)   \r\n"]
[83.602083, "o", "\u001b[2m2025-05-18T15:03:41.991Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.60339, "o", "\u001b[2m2025-05-18T15:03:41.993Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.60562, "o", "\u001b[2m2025-05-18T15:03:41.995Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.609149, "o", "\u001b[2m2025-05-18T15:03:41.999Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.610285, "o", "\u001b[2m2025-05-18T15:03:42.000Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.612401, "o", "\u001b[2m2025-05-18T15:03:42.002Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[83.638228, "o", "\u001b[2m2025-05-18T15:03:42.028Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.639547, "o", "\u001b[2m2025-05-18T15:03:42.029Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.641775, "o", "\u001b[2m2025-05-18T15:03:42.031Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.645403, "o", "\u001b[2m2025-05-18T15:03:42.035Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.646523, "o", "\u001b[2m2025-05-18T15:03:42.036Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.648562, "o", "\u001b[2m2025-05-18T15:03:42.038Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[83.691703, "o", "\u001b[2m2025-05-18T15:03:42.081Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.693052, "o", "\u001b[2m2025-05-18T15:03:42.082Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.696357, "o", "\u001b[2m2025-05-18T15:03:42.086Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.699894, "o", "\u001b[2m2025-05-18T15:03:42.089Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.700972, "o", "\u001b[2m2025-05-18T15:03:42.090Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.70297, "o", "\u001b[2m2025-05-18T15:03:42.092Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[83.756206, "o", "\u001b[2m2025-05-18T15:03:42.146Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.757546, "o", "\u001b[2m2025-05-18T15:03:42.147Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.759766, "o", "\u001b[2m2025-05-18T15:03:42.149Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.763344, "o", "\u001b[2m2025-05-18T15:03:42.153Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.764475, "o", "\u001b[2m2025-05-18T15:03:42.154Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.766503, "o", "\u001b[2m2025-05-18T15:03:42.156Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[83.825961, "o", "\u001b[2m2025-05-18T15:03:42.215Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[83.852525, "o", "\u001b[2m2025-05-18T15:03:42.242Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.854006, "o", "\u001b[2m2025-05-18T15:03:42.243Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.856232, "o", "\u001b[2m2025-05-18T15:03:42.246Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.859674, "o", "\u001b[2m2025-05-18T15:03:42.249Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.860766, "o", "\u001b[2m2025-05-18T15:03:42.250Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.862771, "o", "\u001b[2m2025-05-18T15:03:42.252Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[83.865274, "o", "\u001b[2m2025-05-18T15:03:42.255Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[83.926286, "o", "\u001b[2m2025-05-18T15:03:42.316Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[83.990516, "o", "\u001b[2m2025-05-18T15:03:42.380Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.991921, "o", "\u001b[2m2025-05-18T15:03:42.381Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.993414, "o", "\u001b[2m2025-05-18T15:03:42.383Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[83.994077, "o", "\u001b[2m2025-05-18T15:03:42.383Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.9975, "o", "\u001b[2m2025-05-18T15:03:42.387Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[83.998619, "o", "\u001b[2m2025-05-18T15:03:42.388Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[84.000536, "o", "\u001b[2m2025-05-18T15:03:42.390Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[84.089604, "o", "\u001b[2m2025-05-18T15:03:42.479Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[84.228581, "o", "\u001b[2m2025-05-18T15:03:42.618Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[84.756481, "o", "\u001b[2m2025-05-18T15:03:43.146Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Registering component dynamo/Processor   \r\n"]
[84.757917, "o", "\u001b[2m2025-05-18T15:03:43.147Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Created Processor component   \r\n"]
[84.758077, "o", "\u001b[2m2025-05-18T15:03:43.147Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig.as_args\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Running Processor with args=['--model', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', '--block-size', '64', '--max-model-len', '16384', '--router', 'round-robin']   \r\n"]
[84.799226, "o", "\u001b[2m2025-05-18T15:03:43.189Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Registering component dynamo/VllmWorker   \r\n"]
[84.800145, "o", "\u001b[2m2025-05-18T15:03:43.190Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m created custom lease: Lease { id: 7587886852592923401, cancel_token: CancellationToken { is_cancelled: false } }\r\n"]
[84.801233, "o", "\u001b[2m2025-05-18T15:03:43.191Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Created VllmWorker component with custom lease id 7587886852592923401   \r\n"]
[84.801379, "o", "\u001b[2m2025-05-18T15:03:43.191Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig.as_args\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Running VllmWorker with args=['--model', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', '--block-size', '64', '--max-model-len', '16384', '--enforce-eager', '--max-num-batched-tokens', '16384', '--enable-prefix-caching']   \r\n"]
[84.853639, "o", "\u001b[2m2025-05-18T15:03:43.243Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Registering component dynamo/VllmWorker   \r\n"]
[84.854673, "o", "\u001b[2m2025-05-18T15:03:43.244Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m created custom lease: Lease { id: 7587886852592923405, cancel_token: CancellationToken { is_cancelled: false } }\r\n"]
[84.855775, "o", "\u001b[2m2025-05-18T15:03:43.245Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Created VllmWorker component with custom lease id 7587886852592923405   \r\n"]
[84.855921, "o", "\u001b[2m2025-05-18T15:03:43.245Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig.as_args\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Running VllmWorker with args=['--model', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', '--block-size', '64', '--max-model-len', '16384', '--enforce-eager', '--max-num-batched-tokens', '16384', '--enable-prefix-caching']   \r\n"]
[84.865816, "o", "\u001b[2m2025-05-18T15:03:43.255Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.__init__\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Prefill queue: nats://localhost:4222:vllm   \r\n"]
[84.920874, "o", "\u001b[2m2025-05-18T15:03:43.310Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.__init__\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Prefill queue: nats://localhost:4222:vllm   \r\n"]
[85.147453, "o", "\u001b[2m2025-05-18T15:03:43.537Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Registering component dynamo/VllmWorker   \r\n"]
[85.148782, "o", "\u001b[2m2025-05-18T15:03:43.538Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m created custom lease: Lease { id: 7587886852592923409, cancel_token: CancellationToken { is_cancelled: false } }\r\n"]
[85.150046, "o", "\u001b[2m2025-05-18T15:03:43.540Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Created VllmWorker component with custom lease id 7587886852592923409   \r\n"]
[85.150186, "o", "\u001b[2m2025-05-18T15:03:43.540Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig.as_args\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Running VllmWorker with args=['--model', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', '--block-size', '64', '--max-model-len', '16384', '--enforce-eager', '--max-num-batched-tokens', '16384', '--enable-prefix-caching']   \r\n"]
[85.216914, "o", "\u001b[2m2025-05-18T15:03:43.606Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.__init__\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Prefill queue: nats://localhost:4222:vllm   \r\n"]
[85.325238, "o", "\u001b[2m2025-05-18T15:03:43.715Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Registering component dynamo/VllmWorker   \r\n"]
[85.3263, "o", "\u001b[2m2025-05-18T15:03:43.716Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m created custom lease: Lease { id: 7587886852592923413, cancel_token: CancellationToken { is_cancelled: false } }\r\n"]
[85.327345, "o", "\u001b[2m2025-05-18T15:03:43.717Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Created VllmWorker component with custom lease id 7587886852592923413   \r\n"]
[85.327394, "o", "\u001b[2m2025-05-18T15:03:43.717Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig.as_args\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Running VllmWorker with args=['--model', 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', '--block-size', '64', '--max-model-len', '16384', '--enforce-eager', '--max-num-batched-tokens', '16384', '--enable-prefix-caching']   \r\n"]
[85.39231, "o", "\u001b[2m2025-05-18T15:03:43.782Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.__init__\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Prefill queue: nats://localhost:4222:vllm   \r\n"]
[85.685707, "o", "chat model deepseek-ai/DeepSeek-R1-Distill-Llama-8B removed from the public namespace: public\r\n"]
[86.569054, "o", "Added new chat model deepseek-ai/DeepSeek-R1-Distill-Llama-8B\r\n"]
[86.570071, "o", "+------------+------------------------------------------+-----------+-----------+------------------+\r\n| MODEL TYPE | MODEL NAME                               | NAMESPACE | COMPONENT | ENDPOINT         |\r\n+------------+------------------------------------------+-----------+-----------+------------------+\r\n| chat       | deepseek-ai/DeepSeek-R1-Distill-Llama-8B | dynamo    | Processor | chat/completions |\r\n+------------+------------------------------------------+-----------+-----------+------------------+\r\n"]
[86.681556, "o", "\u001b[2m2025-05-18T15:03:45.071Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mfrontend.start_http_server\u001b[0m\u001b[2m:\u001b[0m [Frontend:1] Starting HTTP server   \r\n"]
[86.682248, "o", "\u001b[2m2025-05-18T15:03:45.072Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mserve_dynamo.web_worker\u001b[0m\u001b[2m:\u001b[0m [Frontend:1] No API routes found, not starting FastAPI server   \r\n\u001b[2m2025-05-18T15:03:45.072Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.web_worker\u001b[0m\u001b[2m:\u001b[0m [Frontend:1] Service is running, press Ctrl+C to stop   \r\n"]
[86.696319, "o", "\u001b[2m2025-05-18T15:03:45.086Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_llm::http::service::service_v2\u001b[0m\u001b[2m:\u001b[0m Starting HTTP service on: 0.0.0.0:8000 \u001b[3maddress\u001b[0m\u001b[2m=\u001b[0m\"0.0.0.0:8000\"\r\n\u001b[2m2025-05-18T15:03:45.086Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_runtime::pipeline::network::tcp::server\u001b[0m\u001b[2m:\u001b[0m tcp transport service on 10.63.133.236:37249\r\n"]
[86.696359, "o", "\u001b[2m2025-05-18T15:03:45.086Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_llm::http::service::discovery\u001b[0m\u001b[2m:\u001b[0m added Chat model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\r\n"]
[92.326469, "o", "\u001b[2m2025-05-18T15:03:50.716Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig._resolve_task\u001b[0m\u001b[2m:\u001b[0m This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.   \r\n"]
[92.425787, "o", "\u001b[2m2025-05-18T15:03:50.815Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig._resolve_task\u001b[0m\u001b[2m:\u001b[0m This model supports multiple tasks: {'reward', 'score', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.   \r\n"]
[92.426166, "o", "\u001b[2m2025-05-18T15:03:50.816Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mcuda.is_async_output_supported\u001b[0m\u001b[2m:\u001b[0m To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used   \r\n"]
[92.429918, "o", "\u001b[2m2025-05-18T15:03:50.819Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mapi_server.build_async_engine_client_from_engine_args\u001b[0m\u001b[2m:\u001b[0m Started engine process with PID 1439   \r\n"]
[92.693568, "o", "\u001b[2m2025-05-18T15:03:51.083Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig._resolve_task\u001b[0m\u001b[2m:\u001b[0m This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.   \r\n"]
[92.693815, "o", "\u001b[2m2025-05-18T15:03:51.083Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mcuda.is_async_output_supported\u001b[0m\u001b[2m:\u001b[0m To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used   \r\n"]
[92.697969, "o", "\u001b[2m2025-05-18T15:03:51.087Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mapi_server.build_async_engine_client_from_engine_args\u001b[0m\u001b[2m:\u001b[0m Started engine process with PID 1442   \r\n"]
[92.904487, "o", "\u001b[2m2025-05-18T15:03:51.294Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig._resolve_task\u001b[0m\u001b[2m:\u001b[0m This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.   \r\n"]
[92.904961, "o", "\u001b[2m2025-05-18T15:03:51.294Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mcuda.is_async_output_supported\u001b[0m\u001b[2m:\u001b[0m To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used   \r\n"]
[92.909782, "o", "\u001b[2m2025-05-18T15:03:51.299Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mapi_server.build_async_engine_client_from_engine_args\u001b[0m\u001b[2m:\u001b[0m Started engine process with PID 1445   \r\n"]
[93.19659, "o", "\u001b[2m2025-05-18T15:03:51.586Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mconfig._resolve_task\u001b[0m\u001b[2m:\u001b[0m This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.   \r\n"]
[93.19687, "o", "\u001b[2m2025-05-18T15:03:51.586Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mcuda.is_async_output_supported\u001b[0m\u001b[2m:\u001b[0m To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used   \r\n"]
[93.200631, "o", "\u001b[2m2025-05-18T15:03:51.590Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mapi_server.build_async_engine_client_from_engine_args\u001b[0m\u001b[2m:\u001b[0m Started engine process with PID 1448   \r\n"]
[93.464621, "o", "\u001b[2m2025-05-18T15:03:51.854Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mconfig.get_diff_sampling_param\u001b[0m\u001b[2m:\u001b[0m Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.   \r\n\u001b[2m2025-05-18T15:03:51.854Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving_chat.__init__\u001b[0m\u001b[2m:\u001b[0m Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}   \r\n"]
[93.55742, "o", "\u001b[2m2025-05-18T15:03:51.947Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserving_completion.__init__\u001b[0m\u001b[2m:\u001b[0m Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}   \r\nProcessor init: round-robin\r\n"]
[93.559524, "o", "\u001b[2m2025-05-18T15:03:51.949Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_runtime::pipeline::network::tcp::server\u001b[0m\u001b[2m:\u001b[0m tcp transport service on 10.63.133.236:46843\r\n"]
[96.249022, "o", "\u001b[2m2025-05-18T15:03:54.638Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.250296, "o", "\u001b[2m2025-05-18T15:03:54.640Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.252482, "o", "\u001b[2m2025-05-18T15:03:54.642Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.279625, "o", "\u001b[2m2025-05-18T15:03:54.669Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.280708, "o", "\u001b[2m2025-05-18T15:03:54.670Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.282758, "o", "\u001b[2m2025-05-18T15:03:54.672Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[96.436905, "o", "\u001b[2m2025-05-18T15:03:54.826Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.438332, "o", "\u001b[2m2025-05-18T15:03:54.828Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.440551, "o", "\u001b[2m2025-05-18T15:03:54.830Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.467705, "o", "\u001b[2m2025-05-18T15:03:54.857Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.468365, "o", "\u001b[2m2025-05-18T15:03:54.858Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[96.468812, "o", "\u001b[2m2025-05-18T15:03:54.858Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.4709, "o", "\u001b[2m2025-05-18T15:03:54.860Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[96.65675, "o", "\u001b[2m2025-05-18T15:03:55.046Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[96.851612, "o", "\u001b[2m2025-05-18T15:03:55.241Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.852984, "o", "\u001b[2m2025-05-18T15:03:55.242Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.855306, "o", "\u001b[2m2025-05-18T15:03:55.245Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.858865, "o", "\u001b[2m2025-05-18T15:03:55.248Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.859973, "o", "\u001b[2m2025-05-18T15:03:55.249Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[96.86197, "o", "\u001b[2m2025-05-18T15:03:55.251Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[97.052631, "o", "\u001b[2m2025-05-18T15:03:55.442Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[97.154389, "o", "\u001b[2m2025-05-18T15:03:55.544Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[97.155676, "o", "\u001b[2m2025-05-18T15:03:55.545Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[97.156581, "o", "\u001b[2m2025-05-18T15:03:55.546Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine.__init__\u001b[0m\u001b[2m:\u001b[0m Initializing a V0 LLM engine (v0.8.4) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,    \r\n"]
[97.157991, "o", "\u001b[2m2025-05-18T15:03:55.547Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[97.16153, "o", "\u001b[2m2025-05-18T15:03:55.551Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[97.162591, "o", "\u001b[2m2025-05-18T15:03:55.552Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2m__init__.vllm_version_matches_substr\u001b[0m\u001b[2m:\u001b[0m Using ai_dynamo_vllm   \r\n"]
[97.164557, "o", "\u001b[2m2025-05-18T15:03:55.554Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m__init__.resolve_current_platform_cls_qualname\u001b[0m\u001b[2m:\u001b[0m Automatically detected platform cuda.   \r\n"]
[97.345862, "o", "\u001b[2m2025-05-18T15:03:55.735Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine.__init__\u001b[0m\u001b[2m:\u001b[0m Initializing a V0 LLM engine (v0.8.4) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,    \r\n"]
[97.352315, "o", "\u001b[2m2025-05-18T15:03:55.742Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mnixl\u001b[0m\u001b[2m:\u001b[0m NIXL is available   \r\n"]
[97.764781, "o", "\u001b[2m2025-05-18T15:03:56.154Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine.__init__\u001b[0m\u001b[2m:\u001b[0m Initializing a V0 LLM engine (v0.8.4) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,    \r\n"]
[98.035568, "o", "\u001b[2m2025-05-18T15:03:56.425Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine.__init__\u001b[0m\u001b[2m:\u001b[0m Initializing a V0 LLM engine (v0.8.4) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,    \r\n"]
[99.050457, "o", "\u001b[2m2025-05-18T15:03:57.440Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mcuda.get_attn_backend_cls\u001b[0m\u001b[2m:\u001b[0m Using Flash Attention backend.   \r\n"]
[99.241068, "o", "\u001b[2m2025-05-18T15:03:57.630Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mcuda.get_attn_backend_cls\u001b[0m\u001b[2m:\u001b[0m Using Flash Attention backend.   \r\n"]
[99.576272, "o", "\u001b[2m2025-05-18T15:03:57.966Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mparallel_state.initialize_model_parallel\u001b[0m\u001b[2m:\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0   \r\n"]
[99.576317, "o", "\u001b[2m2025-05-18T15:03:57.966Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...   \r\n"]
[99.699893, "o", "\u001b[2m2025-05-18T15:03:58.089Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mcuda.get_attn_backend_cls\u001b[0m\u001b[2m:\u001b[0m Using Flash Attention backend.   \r\n"]
[99.86567, "o", "\u001b[2m2025-05-18T15:03:58.255Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mparallel_state.initialize_model_parallel\u001b[0m\u001b[2m:\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0   \r\n"]
[99.865823, "o", "\u001b[2m2025-05-18T15:03:58.255Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...   \r\n"]
[99.935643, "o", "\u001b[2m2025-05-18T15:03:58.325Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mcuda.get_attn_backend_cls\u001b[0m\u001b[2m:\u001b[0m Using Flash Attention backend.   \r\n"]
[100.071204, "o", "\u001b[2m2025-05-18T15:03:58.461Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mweight_utils.download_weights_from_hf\u001b[0m\u001b[2m:\u001b[0m Using model weights format ['*.safetensors']   \r\n"]
[100.2692, "o", "\rLoading safetensors checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"]
[100.312601, "o", "\u001b[2m2025-05-18T15:03:58.702Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mparallel_state.initialize_model_parallel\u001b[0m\u001b[2m:\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0   \r\n"]
[100.312647, "o", "\u001b[2m2025-05-18T15:03:58.702Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...   \r\n"]
[100.355348, "o", "\u001b[2m2025-05-18T15:03:58.745Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mweight_utils.download_weights_from_hf\u001b[0m\u001b[2m:\u001b[0m Using model weights format ['*.safetensors']   \r\n"]
[100.508723, "o", "\u001b[2m2025-05-18T15:03:58.898Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mparallel_state.initialize_model_parallel\u001b[0m\u001b[2m:\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0   \r\n\u001b[2m2025-05-18T15:03:58.898Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...   \r\n"]
[100.556359, "o", "\rLoading safetensors checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"]
[100.690373, "o", "\u001b[2m2025-05-18T15:03:59.080Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mweight_utils.download_weights_from_hf\u001b[0m\u001b[2m:\u001b[0m Using model weights format ['*.safetensors']   \r\n"]
[100.855216, "o", "\u001b[2m2025-05-18T15:03:59.245Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mweight_utils.download_weights_from_hf\u001b[0m\u001b[2m:\u001b[0m Using model weights format ['*.safetensors']   \r\n"]
[100.870929, "o", "\rLoading safetensors checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"]
[101.061699, "o", "\rLoading safetensors checkpoint shards:  50% 1/2 [00:00<00:00,  1.26it/s]"]
[101.147269, "o", "\rLoading safetensors checkpoint shards:   0% 0/2 [00:00<?, ?it/s]"]
[101.340639, "o", "\rLoading safetensors checkpoint shards:  50% 1/2 [00:00<00:00,  1.28it/s]"]
[101.718601, "o", "\rLoading safetensors checkpoint shards:  50% 1/2 [00:00<00:00,  1.18it/s]"]
[102.039924, "o", "\rLoading safetensors checkpoint shards:  50% 1/2 [00:00<00:00,  1.12it/s]"]
[102.487371, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:02<00:00,  1.16s/it]"]
[102.487414, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:02<00:00,  1.11s/it]\r\n"]
[102.489764, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.00it/s]\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.03it/s]\r\n"]
[102.799018, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.02it/s]"]
[102.799064, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.04it/s]\r\n"]
[103.123076, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.00s/it]"]
[103.12312, "o", "\rLoading safetensors checkpoint shards: 100% 2/2 [00:01<00:00,  1.01it/s]\r\n"]
[103.247358, "o", "\u001b[2m2025-05-18T15:04:01.637Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.load_model\u001b[0m\u001b[2m:\u001b[0m Loading weights took 2.98 seconds   \r\n"]
[103.270431, "o", "\u001b[2m2025-05-18T15:04:01.660Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.load_model\u001b[0m\u001b[2m:\u001b[0m Loading weights took 2.71 seconds   \r\n\u001b[2m2025-05-18T15:04:01.660Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.load_model\u001b[0m\u001b[2m:\u001b[0m Loading weights took 2.40 seconds   \r\n"]
[103.300488, "o", "\u001b[2m2025-05-18T15:04:01.690Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mloader.load_model\u001b[0m\u001b[2m:\u001b[0m Loading weights took 2.15 seconds   \r\n"]
[103.464178, "o", "\u001b[2m2025-05-18T15:04:01.854Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Model loading took 14.9889 GiB and 3.407664 seconds   \r\n"]
[103.499692, "o", "\u001b[2m2025-05-18T15:04:01.889Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Model loading took 14.9889 GiB and 2.961104 seconds   \r\n"]
[103.514439, "o", "\u001b[2m2025-05-18T15:04:01.904Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Model loading took 14.9889 GiB and 2.795301 seconds   \r\n"]
[103.701199, "o", "\u001b[2m2025-05-18T15:04:02.091Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mmodel_runner.load_model\u001b[0m\u001b[2m:\u001b[0m Model loading took 14.9889 GiB and 3.674814 seconds   \r\n"]
[105.341931, "o", "\u001b[2m2025-05-18T15:04:03.731Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.determine_num_available_blocks\u001b[0m\u001b[2m:\u001b[0m Memory profiling takes 1.69 seconds\r\nthe current vLLM instance can use total_gpu_memory (44.32GiB) x gpu_memory_utilization (0.90) = 39.89GiB\r\nmodel weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.70GiB; the rest of the memory reserved for KV Cache is 23.12GiB.   \r\n"]
[105.42964, "o", "\u001b[2m2025-05-18T15:04:03.819Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.determine_num_available_blocks\u001b[0m\u001b[2m:\u001b[0m Memory profiling takes 1.70 seconds\r\nthe current vLLM instance can use total_gpu_memory (44.32GiB) x gpu_memory_utilization (0.90) = 39.89GiB\r\nmodel weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.70GiB; the rest of the memory reserved for KV Cache is 23.12GiB.   \r\n"]
[105.466334, "o", "\u001b[2m2025-05-18T15:04:03.856Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.determine_num_available_blocks\u001b[0m\u001b[2m:\u001b[0m Memory profiling takes 1.74 seconds\r\nthe current vLLM instance can use total_gpu_memory (44.32GiB) x gpu_memory_utilization (0.90) = 39.89GiB\r\nmodel weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.70GiB; the rest of the memory reserved for KV Cache is 23.12GiB.   \r\n"]
[105.532683, "o", "\u001b[2m2025-05-18T15:04:03.922Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m # cuda blocks: 2959, # CPU blocks: 512   \r\n\u001b[2m2025-05-18T15:04:03.922Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m Maximum concurrency for 16384 tokens per request: 11.56x   \r\n"]
[105.641638, "o", "\u001b[2m2025-05-18T15:04:04.031Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m # cuda blocks: 2959, # CPU blocks: 512   \r\n"]
[105.641764, "o", "\u001b[2m2025-05-18T15:04:04.031Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m Maximum concurrency for 16384 tokens per request: 11.56x   \r\n"]
[105.702236, "o", "\u001b[2m2025-05-18T15:04:04.092Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m # cuda blocks: 2959, # CPU blocks: 512   \r\n"]
[105.70229, "o", "\u001b[2m2025-05-18T15:04:04.092Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m Maximum concurrency for 16384 tokens per request: 11.56x   \r\n"]
[105.889342, "o", "\u001b[2m2025-05-18T15:04:04.279Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.determine_num_available_blocks\u001b[0m\u001b[2m:\u001b[0m Memory profiling takes 1.76 seconds\r\nthe current vLLM instance can use total_gpu_memory (44.32GiB) x gpu_memory_utilization (0.90) = 39.89GiB\r\nmodel weights take 14.99GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.70GiB; the rest of the memory reserved for KV Cache is 23.12GiB.   \r\n"]
[106.132668, "o", "\u001b[2m2025-05-18T15:04:04.522Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m # cuda blocks: 2959, # CPU blocks: 512   \r\n\u001b[2m2025-05-18T15:04:04.522Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mexecutor_base.initialize_cache\u001b[0m\u001b[2m:\u001b[0m Maximum concurrency for 16384 tokens per request: 11.56x   \r\n"]
[107.450416, "o", "\u001b[2m2025-05-18T15:04:05.840Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine._initialize_kv_caches\u001b[0m\u001b[2m:\u001b[0m init engine (profile, create kv cache, warmup model) took 3.99 seconds   \r\n"]
[107.547377, "o", "\u001b[2m2025-05-18T15:04:05.937Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine._initialize_kv_caches\u001b[0m\u001b[2m:\u001b[0m init engine (profile, create kv cache, warmup model) took 4.03 seconds   \r\n"]
[107.571802, "o", "\u001b[2m2025-05-18T15:04:05.961Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine._initialize_kv_caches\u001b[0m\u001b[2m:\u001b[0m init engine (profile, create kv cache, warmup model) took 4.07 seconds   \r\n"]
[107.714474, "o", "\u001b[2m2025-05-18T15:04:06.104Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.async_init\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] VllmWorker has been initialized   \r\n"]
[107.714715, "o", "\u001b[2m2025-05-18T15:04:06.104Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Starting VllmWorker instance with all registered endpoints   \r\n\u001b[2m2025-05-18T15:04:06.104Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Serving VllmWorker with lease: 7587886852592923409   \r\n"]
[107.714913, "o", "\u001b[2m2025-05-18T15:04:06.104Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mutils.append_dynamo_state\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Skipping append to state file /root/.dynamo/state/dynamo.json because it doesn't exist   \r\n\u001b[2m2025-05-18T15:04:06.104Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Appended lease 7587886852592923409/694d96e3dca87311 to dynamo_VllmWorker   \r\n"]
[107.715885, "o", "\u001b[2m2025-05-18T15:04:06.105Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.create_metrics_publisher_endpoint\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Creating metrics publisher endpoint with lease: <builtins.PyLease object at 0x7f3638f32c10>   \r\n"]
[107.787774, "o", "\u001b[2m2025-05-18T15:04:06.177Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.async_init\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] VllmWorker has been initialized   \r\n"]
[107.787888, "o", "\u001b[2m2025-05-18T15:04:06.177Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Starting VllmWorker instance with all registered endpoints   \r\n"]
[107.787967, "o", "\u001b[2m2025-05-18T15:04:06.177Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Serving VllmWorker with lease: 7587886852592923413   \r\n"]
[107.788148, "o", "\u001b[2m2025-05-18T15:04:06.178Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mutils.append_dynamo_state\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Skipping append to state file /root/.dynamo/state/dynamo.json because it doesn't exist   \r\n"]
[107.788341, "o", "\u001b[2m2025-05-18T15:04:06.178Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Appended lease 7587886852592923413/694d96e3dca87315 to dynamo_VllmWorker   \r\n"]
[107.790509, "o", "\u001b[2m2025-05-18T15:04:06.180Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.create_metrics_publisher_endpoint\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Creating metrics publisher endpoint with lease: <builtins.PyLease object at 0x7f5714109680>   \r\n"]
[107.884282, "o", "\u001b[2m2025-05-18T15:04:06.274Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.async_init\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] VllmWorker has been initialized   \r\n"]
[107.884584, "o", "\u001b[2m2025-05-18T15:04:06.274Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Starting VllmWorker instance with all registered endpoints   \r\n"]
[107.884806, "o", "\u001b[2m2025-05-18T15:04:06.274Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Serving VllmWorker with lease: 7587886852592923405   \r\n\u001b[2m2025-05-18T15:04:06.274Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mutils.append_dynamo_state\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Skipping append to state file /root/.dynamo/state/dynamo.json because it doesn't exist   \r\n\u001b[2m2025-05-18T15:04:06.274Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Appended lease 7587886852592923405/694d96e3dca8730d to dynamo_VllmWorker   \r\n"]
[107.885712, "o", "\u001b[2m2025-05-18T15:04:06.275Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.create_metrics_publisher_endpoint\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Creating metrics publisher endpoint with lease: <builtins.PyLease object at 0x7f2d57b9cc90>   \r\n"]
[108.042752, "o", "\u001b[2m2025-05-18T15:04:06.432Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mllm_engine._initialize_kv_caches\u001b[0m\u001b[2m:\u001b[0m init engine (profile, create kv cache, warmup model) took 4.34 seconds   \r\n"]
[108.077035, "o", "\u001b[2m2025-05-18T15:04:06.466Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mlogging.check_required_workers\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Waiting for more workers to be ready.\r\n Current: 3, Required: 1   \r\nWorkers ready: [7587886852592923409, 7587886852592923413, 7587886852592923405]\r\n"]
[108.080884, "o", "\u001b[2m2025-05-18T15:04:06.470Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Starting Processor instance with all registered endpoints   \r\n"]
[108.080976, "o", "\u001b[2m2025-05-18T15:04:06.470Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Serving Processor with primary lease   \r\n"]
[108.231368, "o", "\u001b[2m2025-05-18T15:04:06.621Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.async_init\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] VllmWorker has been initialized   \r\n"]
[108.231503, "o", "\u001b[2m2025-05-18T15:04:06.621Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Starting VllmWorker instance with all registered endpoints   \r\n\u001b[2m2025-05-18T15:04:06.621Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Serving VllmWorker with lease: 7587886852592923401   \r\n"]
[108.23164, "o", "\u001b[2m2025-05-18T15:04:06.621Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mutils.append_dynamo_state\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Skipping append to state file /root/.dynamo/state/dynamo.json because it doesn't exist   \r\n"]
[108.231708, "o", "\u001b[2m2025-05-18T15:04:06.621Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.worker\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Appended lease 7587886852592923401/694d96e3dca87309 to dynamo_VllmWorker   \r\n"]
[108.23259, "o", "\u001b[2m2025-05-18T15:04:06.622Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.create_metrics_publisher_endpoint\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Creating metrics publisher endpoint with lease: <builtins.PyLease object at 0x7f534ee46460>   \r\n"]
[232.508061, "o", "^C"]
[232.508387, "o", "\u001b[2m2025-05-18T15:06:10.898Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2msighandler.signal\u001b[0m\u001b[2m:\u001b[0m Got signal SIG_INT   \r\n"]
[232.509062, "o", "\u001b[2m2025-05-18T15:06:10.898Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2marbiter.stop\u001b[0m\u001b[2m:\u001b[0m Arbiter exiting   \r\n"]
[232.511675, "o", "\u001b[2m2025-05-18T15:06:10.901Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.signal_handler\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Received signal 15, initiating graceful shutdown   \r\n"]
[232.512814, "o", "\u001b[2m2025-05-18T15:06:10.902Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.main\u001b[0m\u001b[2m:\u001b[0m [Processor:1] Exiting gracefully   \r\n"]
[232.513556, "o", "\u001b[2m2025-05-18T15:06:10.903Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Received signal 15, shutting down   \r\n"]
[232.514447, "o", "\u001b[2m2025-05-18T15:06:10.904Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] VllmWorker shutdown complete   \r\n"]
[232.514908, "o", "\u001b[2m2025-05-18T15:06:10.904Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.<lambda>\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] metrics publisher endpoint created   \r\n"]
[232.517187, "o", "\u001b[2m2025-05-18T15:06:10.907Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Received signal 15, shutting down   \r\n"]
[232.517452, "o", "\u001b[2m2025-05-18T15:06:10.907Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mengine.run_mp_engine\u001b[0m\u001b[2m:\u001b[0m MQLLMEngine terminated\r\nTraceback (most recent call last):\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated   \r\n"]
[232.517493, "o", "Process SpawnProcess-1:\r\n"]
[232.517987, "o", "\u001b[2m2025-05-18T15:06:10.907Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] VllmWorker shutdown complete   \r\n"]
[232.518177, "o", "Traceback (most recent call last):\r\n"]
[232.518583, "o", "\u001b[2m2025-05-18T15:06:10.908Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.<lambda>\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] metrics publisher endpoint created   \r\n"]
[232.518801, "o", "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 600, in run_mp_engine\r\n    raise e\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n"]
[232.518846, "o", "  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated\r\n"]
[232.520995, "o", "\u001b[2m2025-05-18T15:06:10.910Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mengine.run_mp_engine\u001b[0m\u001b[2m:\u001b[0m MQLLMEngine terminated\r\nTraceback (most recent call last):\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated   \r\n"]
[232.521182, "o", "\u001b[2m2025-05-18T15:06:10.911Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Received signal 15, shutting down   \r\nProcess SpawnProcess-1:\r\n"]
[232.521794, "o", "Traceback (most recent call last):\r\n"]
[232.522082, "o", "\u001b[2m2025-05-18T15:06:10.912Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] VllmWorker shutdown complete   \r\n"]
[232.522236, "o", "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 600, in run_mp_engine\r\n    raise e\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n"]
[232.522265, "o", "  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated\r\n"]
[232.52261, "o", "\u001b[2m2025-05-18T15:06:10.912Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.<lambda>\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] metrics publisher endpoint created   \r\n"]
[232.52468, "o", "\u001b[2m2025-05-18T15:06:10.914Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Received signal 15, shutting down   \r\n"]
[232.525559, "o", "\u001b[2m2025-05-18T15:06:10.915Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.shutdown_vllm_engine\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] VllmWorker shutdown complete   \r\n"]
[232.526036, "o", "\u001b[2m2025-05-18T15:06:10.915Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mworker.<lambda>\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] metrics publisher endpoint created   \r\n"]
[232.528462, "o", "\u001b[2m2025-05-18T15:06:10.918Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mengine.run_mp_engine\u001b[0m\u001b[2m:\u001b[0m MQLLMEngine terminated\r\nTraceback (most recent call last):\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated   \r\n\u001b[2m2025-05-18T15:06:10.918Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.signal_handler\u001b[0m\u001b[2m:\u001b[0m [Frontend:1] Received signal 15, initiating graceful shutdown   \r\n"]
[232.528553, "o", "\u001b[2m2025-05-18T15:06:10.918Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mserve_dynamo.web_worker\u001b[0m\u001b[2m:\u001b[0m [Frontend:1] Gracefully shutting down FastAPI process   \r\nProcess SpawnProcess-1:\r\n"]
[232.529262, "o", "Traceback (most recent call last):\r\n"]
[232.529586, "o", "\u001b[2m2025-05-18T15:06:10.919Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_runtime::worker\u001b[0m\u001b[2m:\u001b[0m SIGTERM received, starting graceful shutdown\r\n"]
[232.529726, "o", "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n"]
[232.52988, "o", "  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 600, in run_mp_engine\r\n    raise e\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 595, in run_mp_engine\r\n    engine.start()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 285, in start\r\n    self.cleanup()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 290, in cleanup\r\n    self.ctx.destroy(linger=0)\r\n"]
[232.529988, "o", "  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 322, in destroy\r\n    self.term()\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/zmq/sugar/context.py\", line 264, in term\r\n    super().term()\r\n  File \"_zmq.py\", line 586, in zmq.backend.cython._zmq.Context.term\r\n  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\r\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 576, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated\r\n"]
[233.056449, "o", "[rank0]:[W518 15:06:11.069453013 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"]
[233.098228, "o", "[rank0]:[W518 15:06:11.111281798 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"]
[233.114963, "o", "[rank0]:[W518 15:06:11.128020107 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"]
[233.290708, "o", "[rank0]:[W518 15:06:11.303763501 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"]
[234.136927, "o", "\u001b[2m2025-05-18T15:06:12.526Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._stop\u001b[0m\u001b[2m:\u001b[0m dynamo_Frontend stopped   \r\n"]
[234.422877, "o", "\u001b[2m2025-05-18T15:06:12.812Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._stop\u001b[0m\u001b[2m:\u001b[0m dynamo_Processor stopped   \r\n"]
[234.656594, "o", "\u001b[2m2025-05-18T15:06:13.046Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mserve_dynamo\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:2] Error in main: Event loop stopped before Future completed.   \r\n"]
[235.097308, "o", "\u001b[2m2025-05-18T15:06:13.487Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mserve_dynamo\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:3] Error in main: Event loop stopped before Future completed.   \r\n"]
[235.647186, "o", "\u001b[2m2025-05-18T15:06:14.037Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mserve_dynamo\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:1] Error in main: Event loop stopped before Future completed.   \r\n"]
[236.501963, "o", "\u001b[2m2025-05-18T15:06:14.891Z\u001b[0m \u001b[31mERROR\u001b[0m \u001b[2mserve_dynamo\u001b[0m\u001b[2m:\u001b[0m [VllmWorker:4] Error in main: Event loop stopped before Future completed.   \r\n"]
[238.053897, "o", "\u001b[2m2025-05-18T15:06:16.443Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mwatcher._stop\u001b[0m\u001b[2m:\u001b[0m dynamo_VllmWorker stopped   \r\n"]
[239.319087, "o", "\u001b[?2004h\u001b]0;root@a4u8g-0057: /workspace/examples/llm\u0007root@a4u8g-0057:/workspace/examples/llm# "]
[241.097707, "o", "exit\r\n\u001b[?2004l\rexit\r\n"]
